{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science candidate exercise\n",
    "\n",
    "Thank you for working on the Takt Data Science Exercise! The purpose of this exercise is to allow you to showcase your Data Science skill set in the following areas:\n",
    " * Transforming data\n",
    " * Training and tuning of model parameters\n",
    " * Evaluation of model performance and selecting the optimal model \n",
    " \n",
    "In addition to these areas, weâ€™re interested in seeing how you utilize code to explore a new data set to both explore data and model performance and develop/implement models in our code base. \n",
    "\n",
    "\n",
    "As you work on the exercise, keep in mind that we value reusable code that your teammates can jump into quickly, so please be sure to comment frequently, and let us know which portions of your code you would use for more formal development to showcase your skills in coding style best practices.\n",
    "\n",
    "We also encourage you to use markdown cells to explain your thought process and observations as you move through the exercise. Feel free to use the software and toolbox of your own preference. \n",
    "\n",
    "\n",
    "### Timing and Questions\n",
    "We understand that your time is valuable, while you are welcome to spend as much time as you desire on the exercise, please do not feel obligated to spend more than 2-4 hours.\n",
    " \n",
    "* Questions are encouraged:\n",
    "    * Please feel welcome to contact your TAKT recruiter with any questions you might have for Takt that may help clarify what is expected, or you have questions about the data or other items for our data science team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this exercise, we would like to build collaborative filtering models for recommending product items. Imagine a fast food chain releases a new mobile app allowing its customers to place orders before they even have to walk into the store. There are several opportunities for the app to show recommendations: When a customer first tap on the \"order\" page, we may recommend the first item to be added to the basket (e.g. a burger). After that, items good for pairing with the existing basket could be recommended. For example, if there is a burger already in the order basket, the app may want to recommend fries and/or drinks, rather than recommending another burger.\n",
    "\n",
    "\n",
    "## Input data\n",
    "We provide the artificial transaction history data in `trx_data.csv`. Each row represents a past order. It has two columns - customerId and products (separated by \",\"). The products column contains 1 to 10 product ID(s) of those being purchased, separated by \"|\".\n",
    "\n",
    "Here is an example of the transaction records. You can find customer 0 purchased 1 item, and customer 1 purchased 10 items (some are duplicated).\n",
    "\n",
    "`customerId,products\n",
    "0,20\n",
    "1,2|2|23|68|68|111|29|86|107|152\n",
    "2,111|107|29|11|11|11|33|23\n",
    "3,164|227\n",
    "5,2|2\n",
    "6,144|144|55|267\n",
    "7,136|204|261\n",
    "8,79|8|8|48\n",
    "9,102|2|2|297\n",
    "10,84|77|286|259\n",
    "11,25|127|127\n",
    "12,18|183|288|171|289\n",
    "13,79|8|8|38\n",
    "14,2|2|20|20|20\n",
    "15,251|143\n",
    "`\n",
    "\n",
    "***Things to be aware of about the data.***\n",
    "* The `trx_data.csv` is a log of user purcases, so:\n",
    "    * Users might be found on multiple lines in the csv with different basket items attached.\n",
    "    * The products \"basket\" is not listed in any particular order.\n",
    "        * For example the following data points can be considered as equivelent\n",
    "            * `6,144|144|55|267` \n",
    "            * `6,267|144|55|144`\n",
    "\n",
    "## Models\n",
    "A collaborative filtering model can be built once given a user-item matrix with ratings. \n",
    "For this exercise, we ask tou to build **ONE** of the following two recommender models. \n",
    "\n",
    "### Option 1:\n",
    "* Build a Model that recommends to the user the \"first item\" they may want to place into their \"basket\"\n",
    "  * Input: user - customer ID\n",
    "  * Returns: ranked list of items (product IDs), that the user is most likely to want to put in his/her (empty) \"basket\"\n",
    "\n",
    "### Option 2:\n",
    "* Build a Model for the recommending a \"second item\" after the first item has already been added to the basket.\n",
    "  * Input: (user - customer ID, item - product ID)\n",
    "  * Returns: ranked list of items (product IDs), that the user is most likely to want to put in his/her (empty) \"basket\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Some of the things we will be looking at as we review your submission include:\n",
    "\n",
    "### 1. Data transformation\n",
    "How are you converting the raw data as you prepare a model? Please feel welcome to define and transform the ratings in the way you think would be optimal for generating the best possible product recommendations.\n",
    "\n",
    "### 2. Model selection and development\n",
    "What algorithms did you consider using in order to develop the collaborative filtering model?  Why did you end up selecting the algorithm you ended up using.  How did you end up implementing the selected model?\n",
    "\n",
    "### 3. Validate and evaluate the model performance\n",
    "During the training process, what steps did you use for validating and evaluating the model?\n",
    "* What holdout practices did you use?\n",
    "* What performance metrics did you for validation and testing?\n",
    "* How did you choose to present your results?\n",
    "* What actions did you take or improvements did you make based on your results?\n",
    "\n",
    "### 4. Apply model to test datasets\n",
    "##### Option 1 Model\n",
    "In `recommend_1.csv`, we provide a list of customer IDs. If you select option 1, use this data to generate a csv file that indicates top 10 recommendations for each of the customers. Note the order of the recommended products should be ordered by user preference, with the most preferred item in the beginning.\n",
    "\n",
    "Sample output:\n",
    "\n",
    "`customerId, recommendedProducts\n",
    "1,0|1|2|3|4|5|6|7|8|9\n",
    "2,8|3|1|2|4|7|9|10|11|13\n",
    "3,20|21|22|23|24|25|26|27|28|29\n",
    "...\n",
    "`\n",
    "\n",
    "##### Option 2 Model\n",
    "In recommend_2.csv, we provide a list of customer IDs and their first basket item. If you select option 2, use this data to generate a csv file that indicates top 10 pairing recommendations for each of the customer-basket combinations. Note the order of the recommended products should be ordered by user preference, with the most preferred item in the beginning.\n",
    "\n",
    "Sample output:\n",
    "\n",
    "`customerId, itemId, recommendedProducts\n",
    "1,0,1|2|3|4|5|6|7|8|9|10\n",
    "1,1,0|2|3|4|5|6|7|8|9|10\n",
    "2,8,20|21|22|23|24|25|26|27|28|29\n",
    "...\n",
    "`\n",
    "\n",
    "\n",
    "## Notes on the business use case for evaluation\n",
    "* The goal of this modeling project is to recommend to the user a list of items that they are most likely to purchase (option 1) or add to their existing basket (option 2).  \n",
    "* As you are selecting metrics, please keep in mind that \n",
    " 1. the primary goal is to successfully recommend as many items in your list that they may be inclined to purchase/add, and \n",
    " 2. the secondary goal is that the items are ordered by the user's inclination (the more inclined they are, the higher up in your list of 10 recommendations.\n",
    "\n",
    "## Notes on implementation options\n",
    "\n",
    "For implementation of your modeling solution(s), please feel free to use any method that might be added to production were you to use it for development as a Takt employee. \n",
    "\n",
    "For example:\n",
    "* If you find an open source project licensed under e.g. the Apache 2.0 license or another license open for comercial purposes, you are welcome to use it for your code, or\n",
    "* If you feel like coding up a solution from scratch (assuming you have the time) you are welcome to do that as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## My Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I implemented two solutions in Haskell, a simpler one based on a k Nearest Neighbors approach using Jaccard distance, and a more involved low-rank matrix factorization (MF) approach based on the method of Hu, Koren, and Volinsky in [Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf). The code is separately documented, and can be found in the included project (the `test` folder is a good place to start). I feel that Haskell promotes reproducible results and resuable code better than Python, and this is the sort of development I would like to do at Takt.\n",
    "\n",
    "However you have provided me with a notebook, so I am also includong a more typical data scientist solution (below) in order to walk through my process using Python. This solution again uses the MF method of Hu, Koren, and Volinsky. The main difference between the Haskell and Python solutions is that I wrote the Haskell one from scratch using a stochastic gradient descent (SGD) optimizer, and the Python one uses the alternating least squares (ALS) solver in Ben Frederickson's [implicit](https://github.com/benfred/implicit) library.\n",
    "\n",
    "The `data` folder contains the recommedation files for all three solutions (`data/recommend_1-KNN.csv`, `data/recommend_1-SGD.csv` and `data/recommend_1-ALS.csv`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24381\n",
      "300\n",
      "132638\n"
     ]
    }
   ],
   "source": [
    "customers = list(np.sort(grouped_purchased.CustomerID.unique())) # Get our unique customers\n",
    "products = list(grouped_purchased.StockCode.unique()) # Get our unique products that were purchased\n",
    "quantity = list(grouped_purchased.Quantity) # All of our purchases\n",
    "\n",
    "print len(customers)\n",
    "print len(products)\n",
    "print len(quantity)\n",
    "\n",
    "rows = grouped_purchased.CustomerID.astype('category', categories = customers).cat.codes \n",
    "# Get the associated row indices\n",
    "cols = grouped_purchased.StockCode.astype('category', categories = products).cat.codes \n",
    "# Get the associated column indices\n",
    "purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that most of the data processing work was done in Haskell, since I implemented the other two solutions first. In particular `data/trx_unbatched.csv` was generated by the `convert` function in `Main.hs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId  itemId  quantity\n",
       "0           0       1         2\n",
       "1           0      13         1\n",
       "2           0      19         3\n",
       "3           0      20         1\n",
       "4           0      31         2"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases = pd.read_csv(\"data/trx_unbatched.csv\")\n",
    "purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead of representing an explicit rating, the purchase quantity can represent a \"confidence\" in terms of how strong the interaction was. Items with a larger number of purchases by a customer can carry more weight in our ratings matrix of purchases. \n",
    "\n",
    "Our last step is to create the sparse ratings matrix of users and items utilizing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24429\n",
      "300\n",
      "133585\n"
     ]
    }
   ],
   "source": [
    "customers = list(np.sort(purchases.customerId.unique())) # Get our unique customers\n",
    "products = list(purchases.itemId.unique()) # Get our unique products that were purchased\n",
    "quantity = list(purchases.quantity) # All of our purchases\n",
    "\n",
    "print len(customers)\n",
    "print len(products)\n",
    "print len(quantity)\n",
    "\n",
    "rows = purchases.customerId.astype('category', categories = customers).cat.codes \n",
    "# Get the associated row indices\n",
    "cols = purchases.itemId.astype('category', categories = products).cat.codes \n",
    "# Get the associated column indices\n",
    "purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check our final matrix object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24429x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 133585 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have 24429 customers with 300 items. For these user/item interactions, 133585 of these items had a purchase. In terms of sparsity of the matrix, that means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.17723470738329"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_size = purchases_sparse.shape[0]*purchases_sparse.shape[1] # Number of possible interactions in the matrix\n",
    "num_purchases = float(len(purchases_sparse.nonzero()[0])) # Number of items interacted with\n",
    "sparsity = 100.0*(1 - (num_purchases/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "98.2% of the interaction matrix is sparse. For collaborative filtering to work, the maximum sparsity you could get away with would probably be about 99.5% or so. We are well below this, so we should be able to get decent results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Typically in ML applications, you test the model by creating a test set completely separate from the training set. Usually this is fairly simple: just take a random sample of the training example rows in our feature matrix and separate it away from the training set. \n",
    "\n",
    "With collaborative filtering it's slightly more involved because you need all of the user/item interactions to find the proper dimensions for the matrix. One approach is to hide a certain percentage of the user/item interactions from the model during the training phase chosen at random. Then, check during the test phase how many of the items that were recommended the user actually ended up purchasing in the end. Ideally, you would ultimately test your recommendations with some kind of A/B test or utilizing data from a time series where all data prior to a certain point in time is used for training while data after a certain period of time is used for testing. \n",
    "\n",
    "Our test set is an exact copy of our original data. The training set, however, will mask a random percentage of user/item interactions and act as if the user never purchased the item. We then check in the test set which items were recommended to the user that they ended up actually purchasing. If the users frequently ended up purchasing the items most recommended to them by the system, we can conclude the system seems to be working. \n",
    "\n",
    "As an additional check, we can compare our system to simply recommending the most popular items to every user. This will be our baseline. This method of testing isn't necessarily the \"correct\" answer, because it depends on how you want to use the recommender system. However, it is a practical way of testing performance I will use for this example.\n",
    "\n",
    "Now that we have a plan on how to separate our training and testing sets, let's create a function that can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_train(ratings, pct_test = 0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "    \n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "    training set for later comparison to the test set, which contains all of the original ratings. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "    that originally had interaction set back to zero.\n",
    "    \n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "    compares with the actual interactions.\n",
    "    \n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This will return our training set, a test set that has been binarized to 0/1 for purchased/not purchased, and a list of which users had at least one item masked. We will test the performance of the recommender system on these users only. I am masking 10% of the user/item interactions for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "product_train, product_test, product_users_altered = make_train(purchases_sparse, pct_test = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing ALS for Implicit Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have our training and test sets finished, we can move on to implementing the algorithm. If you look at the [Hu, Koren, and Volinsky](http://yifanhu.net/PUB/cf.pdf) paper you can see the key equations will we need to implement. First, we have our ratings matrix which is sparse (represented by the product_train sparse matrix object). We need to turn this into a confidence matrix (from page 4): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{equation} \n",
    "C_{ui} = 1 + \\alpha r_{ui}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Where $C_{ui}$ is the confidence matrix for our users $u$ and our items $i$. The $\\alpha$ term represents a linear scaling of the rating preferences (in our case number of purchases) and the $r_{ui}$ term is our original matrix of purchases. The paper suggests $\\alpha = 40$  as a good starting point. I found $\\alpha = 15$ to work well for this problem. Note that the parameter is rather sensitive and $\\alpha = 40$ led to floating point overflow errors in both the Haskell and Python versions.\n",
    "\n",
    "After taking the derivative of equation 3 in the paper, we can minimize the cost function for our users $U$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{equation} \n",
    "x_{u} = (Y^{T}C^{u}Y + \\lambda I)^{-1}Y^{T}C^{u}p(u)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The authors note you can speed up this computation through some linear algebra that changes this equation to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{equation} \n",
    "x_{u} = (Y^{T}Y + Y^{T}(C^{u}-I)Y + \\lambda I)^{-1}Y^{T}C^{u}p(u)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that we can now precompute the $Y^{T}Y$ portion without having to iterate through each user $u$. We can derive a similar equation for our items:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{equation} \n",
    "y_{i} = (X^{T}X + X^{T}(C^{i}-I)X + \\lambda I)^{-1}X^{T}C^{i}p(i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These will be the two equations we will iterate back and forth between until they converge. We also have a  Tikhonov regularization term $\\lambda$ that can help prevent overfitting during the training stage as well, along with our binarized preference matrix $p$ which is just 1 where there was a purchase (or interaction) and zero where there was not. \n",
    "\n",
    "Now that the math part is out of the way, we can look at Python implementations. [Chris Johnson's implicit-mf](https://github.com/MrChrisJohnson/implicit-mf/blob/master/mf.py) code is a helpful, but barebones Python implementation. For this version I'll use Ben Frederickson's [implicit](https://github.com/benfred/implicit) library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implicit: Fast Python Collaborative Filtering for Implicit Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Chris Johnson's version doesn't have an explicit parameter for the weighting $\\alpha$ and assumes you are doing this to the ratings matrix before using it as an input.\n",
    "\n",
    "I tuned the ALS hyperparameters a bit but could have been more methodical. Ideally, you would have separate train, cross-validation, and test sets to avoid overfitting while tuning the hyperparameters, but this setup is adequate for a baseline comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((product_train*alpha).astype('double'), \n",
    "                                                          factors=10, \n",
    "                                                          regularization = 0.1, \n",
    "                                                         iterations = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can investigate ratings for a particular user by taking the dot product between the user and item vectors ($U$ and $V$). Let's look at our first user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.18913761,  1.02650357,  0.7717409 ,  0.61418907,  0.69920199])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vecs.dot(item_vecs.T)[0,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluating the Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A commonly used metric for evaluating binary classifiers is AUC, which is the area under the [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (or ROC) curve. A greater area under the curve essentially means users are purchasing items we recommend more frequently. \n",
    "\n",
    "Usually AUC is used in more typical binary classification problems to identify how well a model can predict a positive example vs. a negative one. It is also well-suited to our purposes of ranking recommendations, though one could use an [F1 score](https://en.wikipedia.org/wiki/F1_score). I actually used F1 scores to assess the two models I wrote in Haskell (see the `test` directory), mainly because it's easier to implement.\n",
    "\n",
    "So, to compute AUC we need to write a function that can calculate a mean area under the curve (AUC) for any user that had at least one masked item. As a benchmark, we will also calculate what the mean AUC would have been if we had simply recommended the most popular items. Popularity tends to be hard to beat in most recommender system problems, so it makes a good comparison. \n",
    "\n",
    "First, let's make a simple function that can calculate our AUC. Scikit-learn has one we can alter a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    - predictions: your prediction output\n",
    "    \n",
    "    - test: the actual target result you are comparing to\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, utilize this helper function inside of a second function that will calculate the AUC for each user in our training set that had at least one item masked. It also calculates AUC for the most popular items for our users to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model \n",
    "    \n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "    \n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "    \n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = [] # To store AUC scores from the popular items classifier\n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users: # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user,:].toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "        # Select binarized yes/no interaction pairs from the original full data that align with the same pairs in training \n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC rounded to three decimal places for both test and popularity benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now use this function to see how our recommender system is doing. To use this function, we will need to transform our output from the ALS function to csr_matrix format and transpose the item vectors. The original pure Python version output the user and item vectors into the correct format already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.693, 0.675)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC for our recommender system\n",
    "calc_mean_auc(product_train, product_users_altered, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], product_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that our recommender system beat popularity by a small amount. Our system had a mean AUC of 0.693, while the popular item benchmark had a lower AUC of 0.675. As the system scales (especially as more products are added) I would expect the MF version to continue to improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = grouped_purchased.groupby(\"StockCode\").agg({\"Quantity\": np.sum})\n",
    "sorted = grouped.sort_values([\"Quantity\"], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA40AAAJFCAYAAAB5mBJUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11416ef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sorted.plot(kind='bar', title =\"Purchases\", figsize=(15, 10), legend=False, fontsize=12)\n",
    "ax.set_ylabel(\"# Purchases\", fontsize=12)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we can create a function that will return a list of the item recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rec_items(customer_id, mf_train, user_vecs, item_vecs, customer_list, item_list, num_items = 10):\n",
    "    '''\n",
    "    This function returns the top recommended items to our users \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    customer_id - Input the customer's id number that you want to get recommendations for\n",
    "    \n",
    "    mf_train - The training matrix you used for matrix factorization fitting\n",
    "    \n",
    "    user_vecs - the user vectors from your fitted matrix factorization\n",
    "    \n",
    "    item_vecs - the item vectors from your fitted matrix factorization\n",
    "    \n",
    "    customer_list - an array of the customer's ID numbers that make up the rows of your ratings matrix \n",
    "                    (in order of matrix)\n",
    "    \n",
    "    item_list - an array of the products that make up the columns of your ratings matrix\n",
    "                    (in order of matrix)\n",
    "    \n",
    "\n",
    "    num_items - The number of items you want to recommend in order of best recommendations. Default is 10. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    - The top n recommendations chosen based on the user/item vectors \n",
    "    '''\n",
    "    \n",
    "    cust_ind = np.where(customer_list == customer_id)[0][0] # Returns the index row of our customer id\n",
    "\n",
    "    rec_vector = user_vecs[cust_ind,:].dot(item_vecs.T) # Get dot product of user vector and all item vectors\n",
    "\n",
    "    product_idx = np.argsort(rec_vector)[::-1][:num_items] # Sort the indices of the items into order \n",
    "    # of best recommendations\n",
    "    rec_list = [] # start empty list to store items\n",
    "    for index in product_idx:\n",
    "        code = item_list[index]\n",
    "        rec_list.append(code) \n",
    "\n",
    "    return rec_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This will retrieve the $N$ highest ranking dot products between our user and item vectors for a particular user. Note that *items already purchased may still be recommended to the user*. I chose this because the problem was framed as a recommender for food items (as opposed to say books, which are usually purchased once), and there aren't many items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 17, 33, 1, 69, 63, 13, 21, 60]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_arr = np.array(customers) # Array of customer IDs from the ratings matrix\n",
    "products_arr = np.array(products) # Array of product IDs from the ratings matrix\n",
    "rec_items(0, product_train, user_vecs, item_vecs, customers_arr, products_arr,\n",
    "                       num_items = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our recommendations out to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_recs(x):\n",
    "    recs = rec_items(x, product_train, user_vecs, item_vecs, customers_arr, products_arr, num_items = 10)\n",
    "    return '|'.join(map(str,recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"data/recommend_1.csv\")\n",
    "#recs = test.applymap(process_recs)\n",
    "test = pd.read_csv(\"data/trx1.ab\").iloc[:, [0]]\n",
    "\n",
    "recs = test.applymap(process_recs)\n",
    "out = pd.concat([test,recs],axis=1)\n",
    "out.columns = [\"customerId\", \"recommendedProducts\"]\n",
    "out.to_csv(\"data/recommend_1-ALSshort.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Submission\n",
    "\n",
    "When you have completed your exercise, please email the following to your recruiter:\n",
    "1. This Raw .ipynb file with the stdout included under your cells\n",
    "1. An HTML export of the .ipynb file to ensure that no output is lost in submitting the raw .ipynb\n",
    "    * [File] > [Download as] > [HTML (.html)]\n",
    "1. The output CSV file, with format passing the below sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission file has passed the format check!\n"
     ]
    }
   ],
   "source": [
    "### The following is used to validate your submission format\n",
    "FILE_NAME = \"/Users/cem3394/Documents/code/haskell/recommenders/alternate/data/recommend_1-KNN.csv\" # Put your filename here\n",
    "\n",
    "submission_file = open(FILE_NAME)\n",
    "header = submission_file.readline()\n",
    "columns = header.strip().split(\",\")\n",
    "numColumns = 2\n",
    "# check columns in the header\n",
    "if len(columns) == 2 and columns[0] == \"customerId\" and columns[1] == \"recommendedProducts\":\n",
    "  # The header matches the requirement of Option 1\n",
    "  pass\n",
    "elif len(columns) == 3 and columns[0] == \"customerId\" and columns[2] == \"itemId\" and columns[2] == \"recommendedProducts\":\n",
    "  # The header matches the requirement of Option 2\n",
    "  numColumns = 3\n",
    "else:\n",
    "  raise Exception(\"The header does not match the required format!\")\n",
    "  \n",
    "# check data    \n",
    "for (i, l) in enumerate(submission_file):\n",
    "  fields = l.strip().split(\",\")\n",
    "  if len(fields) != numColumns:\n",
    "    raise Exception(\"Line %d does not have %d comma-separated fields!\" %(i+1, numColumns))      \n",
    "  if len(fields[-1].split(\"|\")) != 10:\n",
    "    raise Exception(\"Line %d does not have 10 recommended products!\" %(i+1))\n",
    "\n",
    "if i != 999:\n",
    "  raise Exception(\"The submission file has %d lines (1000 expected)\" %(i+1))\n",
    "\n",
    "print(\"Your submission file has passed the format check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
